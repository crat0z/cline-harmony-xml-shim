# ðŸ¤–ðŸ’¥ **cline-harmony-xml-shim** ðŸ’¥ðŸ¤–

### *âœ¨ The Screaming, Glitter-Soaked XML Translator for Cline Ã— llama.cpp (gpt-oss-20b/120b) âœ¨*

**â€” This README is 200% AI-written. Expect em-dashes, emoji riots, and bullet lists that never end. â€”**

---

## ðŸ§¨ WHY DOES THIS EXIST (AND WHY IS IT YELLING)

* Cline: â€œGive me **XML tool calls** in `content` plz.â€
* gpt-oss on llama.cpp: â€œHereâ€™s some **native `tool_calls`**, and alsoâ€”surpriseâ€”**reasoning deltas**.â€
* **You**: â€œMy VS Code has become a haunted house.â€
* **This shim**: *Holds your latte.* Converts native tool calls â†’ **Cline XML** on the fly, streams cleanly, and **always** sends `[DONE]`. âœ…

> **TL;DR**: You keep your model. You keep llama.cpp. Cline gets the XML it craves. Everybody stops crying. ðŸš€

---

## ðŸ”§ WHAT IT DOES (LOUD EDITION)

* ðŸ”„ **Translates** OpenAI-style `tool_calls` â†’ **Cline XML** in `delta.content`
* ðŸ§µ **Streams** like a champ (SSE) â€” role delta first, chunks next, `[DONE]` at the end
* ðŸ§  **Rescues** empty replies:

  * Promote **reasoning â†’ content** (optional)
  * Or synthesize `<ask_followup_question>â€¦</ask_followup_question>` (optional)
* ðŸ§­ **Understands modes**: detects **PLAN** vs **ACT** and applies per-mode knobs
* ðŸ§° **Tames tools**: strict mode, browser fallbacks, aliasesâ€¦ you name it
* ðŸ“ˆ **Logging that slaps**: DEBUG body snapshots (messages elided), full stream tees to files, neat turn summaries
* âš™ï¸ **Sampling sanity**:

  * **Strips client sampling by default** (temperature/top\_p/top\_k/etc.)
  * **Cache reuse ON** (shim sends `cache_prompt=true`)
  * **Reasoning format auto** (shim injects `reasoning_format="auto"` so you actually get `reasoning_content`)
  * **Strip client sampling ON** (ignores client `temperature/top_p/top_k/...`)
  * **Tool call options locked** (shim sets `parallel_tool_calls=false`, `parse_tool_calls=true` when advertising tools)
  * `reasoning_effort` is opt-in via flags (see below)

### 0) **Model + llama.cpp** â€” Do this first or nothing works, lol

> **You MUST** run `llama-server` with **`--jinja` enabled**. The shim force-feeds `reasoning_format=auto` in every request, so you still get those juicy `reasoning_content` deltas.
> Also: **highly recommend** `--cache-reuse 256` (models be chunky; reuse that prompt cache ðŸ”¥).

**Windows / PowerShell** (adjust paths for your life choices):

```powershell
# llama-server on 8080 (default) â€” gpt-oss-20b example
.\llama-server.exe `
  -m "C:\ai\models\gpt-oss-20b-mxfp4.gguf" `
  --host 127.0.0.1 --port 8080 `
  --jinja `
  --cache-reuse 256 `
  -a gpt-oss-20b
```

**Linux / macOS** (zsh/bash vibe):

```bash
./llama-server \
  -m "/opt/models/gpt-oss-20b-mxfp4.gguf" \
  --host 127.0.0.1 --port 8080 \
  --jinja \
  --cache-reuse 256 \
  -a gpt-oss-20b
```

### 1) **Run the shim** â€” the glittery translator ðŸª„

> Weâ€™ll listen on **127.0.0.1:8787** (not 8081, weâ€™re âœ¨differentâœ¨), and talk to llama-server on **8080**.

```powershell
python ./cline-harmony-xml-shim.py `
  --host 127.0.0.1 `
  --port 8787 `
  --upstream http://127.0.0.1:8080
```

* Defaults (spicy and sensible):

  * **Cache reuse ON** (shim sends `cache_prompt=true`)
  * **Reasoning format auto** (shim injects `reasoning_format="auto"` so you actually get `reasoning_content`)
  * **Strip client sampling ON** (ignores client `temperature/top_p/top_k/...`)
  * `reasoning_effort` is opt-in via flags (see below)

### 2) **Point Cline at the shim**

* **Base URL**: `http://127.0.0.1:8787`
* **Model name**: `gpt-oss-20b` (llama-server doesnâ€™t care, but tidy vibes matter)

---

## âš¡ POPULAR COMMANDS (COPY/PASTE CHAOS)

**Super verbose + stream tees (diagnostic mode, bring ibuprofen):**

```powershell
python ./cline-harmony-xml-shim.py `
  --host 127.0.0.1 --port 8787 `
  --upstream http://127.0.0.1:8080 `
  --log-level DEBUG --log-body --trace-stream `
  --dump-upstream up.log --dump-downstream down.log
```

**Strict XML (reject unknown tools) + synthesize fallback:**

```powershell
python ./cline-harmony-xml-shim.py `
  --host 127.0.0.1 --port 8787 `
  --upstream http://127.0.0.1:8080 `
  --strict-xml `
  --synthesize-empty-xml
```

**Per-mode sauce (PLAN cold, ACT spicy) + reasoning effort:**

```powershell
python ./cline-harmony-xml-shim.py `
  --host 127.0.0.1 --port 8787 `
  --upstream http://127.0.0.1:8080 `
  --set-sampling-plan "temperature=0.2" `
  --set-sampling-act "temperature=0.7" `
  --reasoning-effort-act high
```

**Make every turn hit a tool (the compliance switch):**

```powershell
python ./cline-harmony-xml-shim.py `
  --host 127.0.0.1 --port 8787 `
  --upstream http://127.0.0.1:8080 `
  --force-tool-calls
```

<sub>Bonus: when the user screams for a condense, the shim shoves a `condense` tool choice upstream automatically.</sub>

> **Heads-up:** On current llama.cpp builds, `--force-tool-calls` disables reasoning deltas. Grab the patched fork: [`llama.cpp:gpt-oss-reasoning-tool-call`](https://github.com/crat0z/llama.cpp/tree/gpt-oss-reasoning-tool-call).

**Flip defaults just this run (you absolute renegade):**

```powershell
# Disable cache reuse for one session
python ./cline-harmony-xml-shim.py ... --no-cache-reuse

# Allow client sampling to seep through
python ./cline-harmony-xml-shim.py ... --no-strip-client-sampling
```

---

## ðŸ§  HOW IT MAGICALLY WORKS (LOUD BUT TRUE)

* Proxies `/v1/chat/completions` â†’ llama-server
* Buffers streaming deltas:

  * `delta.tool_calls` (coalesced per index)
  * `delta.reasoning_content` (for optional promotion)
  * `delta.content` (flushed every N bytes or on newline)
* When tools finish (`finish_reason: "tool_calls"` or stream end):

  * Converts each call â†’ **Cline XML**:

    ```
    <tool_name>
      <param1>...</param1>
      <param2>...</param2>
    </tool_name>
    ```
  * Emits that as **`delta.content`** downstream, keeps the party going ðŸŽ‰
* If **no assistant content** was produced:

  * Optionally **promote reasoning** â†’ content, **or**
  * **Synthesize** `<ask_followup_question>` so Cline doesnâ€™t faceplant
* Always emits **`[DONE]`**. No ghost sockets. No tears. ðŸ˜Œ

---

## ðŸ§© TOOLS & ALIASES (BECAUSE MODELS ARE LITTLE GREMLINS)

* **Known tags** include (examples, not exhaustive):
  `read_file`, `write_to_file`, `replace_in_file`, `search_files`, `list_files`,
  `execute_command`, `list_code_definition_names`,
  `ask_followup_question`, `attempt_completion`, `new_task`, `plan_mode_respond`,
  `use_mcp_tool`, `access_mcp_resource`, `condense`

* **Aliases** (because YOLO):

  * `exec|run_command|shell|bash|powershell` â†’ `execute_command`
  * `ls|list` â†’ `list_files`
  * `read|write|replace|search` â†’ the obvious file tools
  * Browser-ish names (`open_url`, `navigate`, `web.search`, etc.) â†’
    `<use_mcp_tool><server_name>browser</server_name>â€¦</use_mcp_tool>` by default

* **Strictness menu**:

  * `--strict-xml` â†’ unknown tools **rejected**
  * *(default)* unknown tools â†’ `<use_mcp_tool server="unknown">â€¦</use_mcp_tool>`
    *(Yes, thatâ€™s chaotic neutral.)*

---

## ðŸ§° SAMPLING / REASONING KNOBS (TURN THEM. FEEL POWER.)

* **Default**: **strip client sampling ON**

  * Drop `temperature`, `top_p`, `top_k`, etc. (Cline can beâ€¦ opinionated)
  * Disable with `--no-strip-client-sampling`
* **Cache reuse**: **ON by default** â†’ sends `cache_prompt=true` upstream

  * Disable with `--no-cache-reuse`
* **Reasoning effort** (llama-server top-level):

  * `--reasoning-effort low|medium|high`
  * Or per-mode: `--reasoning-effort-plan â€¦`, `--reasoning-effort-act â€¦`

---

## ðŸ•µï¸ LOGGING YOUâ€™LL ACTUALLY READ (OR NOT)

* `--log-level DEBUG --log-body --trace-stream` â†’ maximal chattiness
* `--dump-upstream up.log` / `--dump-downstream down.log` â†’ raw SSE tees
* **Turn summary** logs (INFO):
  `mode=PLAN|ACT sampling={â€¦} tools_injected=âœ“ converted_xml=âœ“ content_emitted=âœ“ cache_prompt=âœ“ reasoning_effort=high`

---

## ðŸ§¯ TROUBLESHOOTING (THE â€œWHY IS IT LIKE THISâ€ SECTION)

* **â€œCline tried to use `write_to_file` without `path`.â€**
  â†’ The model emitted a tool call with missing/renamed args. The shim didnâ€™t drop them; it converts what it gets.
  **Fix vibes:** Improve examples, enable tool extraction, or add arg-synonym normalization in code (map `filepath`â†’`path`, `text`â†’`content`).

* **â€œNo assistant messages.â€**
  â†’ Upstream only streamed reasoning.
  **Use:** `--synthesize-empty-xml` or `--promote-reasoning-if-empty`.

* **â€œPremature close.â€**
  â†’ Shim always sends `[DONE]`. Check exceptions + your tee logs.

---

## ðŸ—ºï¸ ROADMAP (100% ASPIRATIONAL, 0% GUARANTEED)

* Smarter arg normalization (auto-fix `filepath`/`text`/etc.)
* Optional hard XML grammar guardrails
* Prompt-tool catalog extraction + validation
* â€œDeveloper modeâ€ logs that roast your config ðŸ”¥

---

## ðŸ FINAL BOSS CHECKLIST (DO THESE OR PERISH)

* âœ… `llama-server` running with **`--jinja --cache-reuse 256`**
* âœ… Shim running on **127.0.0.1:8787** pointing to **[http://127.0.0.1:8080](http://127.0.0.1:8080)**
* âœ… Cline configured to hit the shim, model name `gpt-oss-20b`
* âœ… Optional: `--synthesize-empty-xml` for when the model only **thinks** and never **speaks**

> If you made it this far through the migraineâ€”congrats. You are now the proud owner of a **loud** but **effective** Cline Ã— llama.cpp bridge. Go forth and ship XML. ðŸš€
